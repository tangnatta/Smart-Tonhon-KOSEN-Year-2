{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.google.com/search?q=what&tbm=isch\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from multiprocessing import cpu_count\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class google_webscraping:\n",
    "    \"\"\"Images Web Scraper for Google Images\n",
    "    \"\"\"\n",
    "    def __init__(self, path: str, query: str, total_img: int) -> None:\n",
    "        \"\"\"Images Web Scraper for Google Images\n",
    "\n",
    "        Args:\n",
    "            path (str): Path for images to downlaod\n",
    "            query (str): search query\n",
    "            total_img (int): total images to download\n",
    "        \"\"\"\n",
    "        self.GOOGLE_BASE: str = \"https://www.google.com\"\n",
    "        self.query: str = query\n",
    "        self.total_img: int = total_img\n",
    "        self.IMG_PER_PAGE: int = 20\n",
    "        self.BASE_PATH = os.getcwd()\n",
    "        if not os.path.exists(path):\n",
    "            # assert False, \"Path does not exist\"\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "        self.path: str = path\n",
    "        os.chdir(os.path.join(os.getcwd(), self.path))\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the scraper and return the path back to the base path\"\"\"\n",
    "        os.chdir(self.BASE_PATH)\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"start image scraping from google images and save in to the path with multiple threads\n",
    "        \"\"\"\n",
    "        procs = []\n",
    "        pool = ThreadPool(processes=cpu_count())\n",
    "        for start_index in range(0, self.total_img, self.IMG_PER_PAGE):\n",
    "            proc = pool.apply_async(self.run, args=(start_index,))\n",
    "            procs.append(proc)\n",
    "\n",
    "        # complete the processes\n",
    "        for proc in procs:\n",
    "            proc.get()\n",
    "\n",
    "    def run(self, start_index: int) -> None:\n",
    "        \"\"\"run the scraper for the given start index and download the images to the path\n",
    "\n",
    "        Args:\n",
    "            start_index (int): start index for the images to download\n",
    "        \"\"\"\n",
    "        search_url: str = self.get_search_url(self.query, start_index)\n",
    "        soup: BeautifulSoup = self.get_soup(search_url)\n",
    "        image_urls: list = self.get_image_urls(soup)[:self.total_img]\n",
    "        self.download_images(image_urls)\n",
    "\n",
    "    def download_images(self, image_urls: list) -> None:\n",
    "        \"\"\"download images from the given image urls\n",
    "\n",
    "        Args:\n",
    "            image_urls (list): images urls to download\n",
    "        \"\"\"\n",
    "        for url in image_urls:\n",
    "            response: requests.Response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                img = cv2.imdecode(np.frombuffer(\n",
    "                    response.content, np.uint8), cv2.IMREAD_UNCHANGED)\n",
    "                cv2.imwrite(filename=self.query+'_' +\n",
    "                            url.split(':')[-1]+'.png', img=img)\n",
    "            else:\n",
    "                print(\"FAILED\", url)\n",
    "\n",
    "    def get_soup(self, url: str) -> BeautifulSoup:\n",
    "        \"\"\"get the soup (html for filtered) from the given url using requests and BeautifulSoup\n",
    "\n",
    "        Args:\n",
    "            url (str): url to get the soup\n",
    "\n",
    "        Returns:\n",
    "            BeautifulSoup: BeautifulSoup object for the given url\n",
    "        \"\"\"\n",
    "        response: requests.Response = requests.get(url)\n",
    "        return BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    def get_image_urls(self, soup: BeautifulSoup) -> list:\n",
    "        \"\"\"filter only the images urls from the given soup (Google search page - html)\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): BeautifulSoup object from get_soup method\n",
    "\n",
    "        Returns:\n",
    "            list: images urls list from the given soup\n",
    "        \"\"\"\n",
    "        image_urls: list = []\n",
    "        for img in soup.find_all(\"img\"):\n",
    "            if img.has_attr(\"src\"):\n",
    "                if not img[\"src\"].startswith(\"http\"):\n",
    "                    continue\n",
    "                image_urls.append(img[\"src\"])\n",
    "        return image_urls\n",
    "\n",
    "    def get_search_url(self, query: str, start_index: int) -> str:\n",
    "        \"\"\"get the search url for the given query and start index from the google images\n",
    "\n",
    "        Args:\n",
    "            query (str): search query\n",
    "            start_index (int): start index for the images \n",
    "\n",
    "        Returns:\n",
    "            str: url with given query and start index\n",
    "        \"\"\"\n",
    "        return self.GOOGLE_BASE + \"/search?q={query}&tbm=isch&start={start_index}\".format(query=query, start_index=start_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file <_io.BytesIO object at 0x0000024AFF7AB3D0>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m google_webscraping(path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpirate_ship_images\u001b[39;49m\u001b[39m\"\u001b[39;49m, query\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpirate ship\u001b[39;49m\u001b[39m\"\u001b[39;49m, total_img\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[7], line 17\u001b[0m, in \u001b[0;36mgoogle_webscraping.__init__\u001b[1;34m(self, path, query, total_img)\u001b[0m\n\u001b[0;32m     15\u001b[0m soup: BeautifulSoup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_soup(search_url)\n\u001b[0;32m     16\u001b[0m image_urls: \u001b[39mlist\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_image_urls(soup)[:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_img]\n\u001b[1;32m---> 17\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload_images(image_urls)\n",
      "Cell \u001b[1;32mIn[7], line 36\u001b[0m, in \u001b[0;36mgoogle_webscraping.download_images\u001b[1;34m(self, image_urls)\u001b[0m\n\u001b[0;32m     22\u001b[0m     response: requests\u001b[39m.\u001b[39mResponse \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url)\n\u001b[0;32m     23\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[0;32m     24\u001b[0m         \u001b[39m# filename: re.Match = re.search(r\"/([\\w_-]+[.](jpg|gif|png))$\", url)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m         \u001b[39m# if not filename:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[39m# with open(self.query+'_'+url.split(':')[-1]+'.jpg', \"wb\") as f:\u001b[39;00m\n\u001b[0;32m     34\u001b[0m         \u001b[39m#     f.write(response.raw)\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m         img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(response\u001b[39m.\u001b[39;49mraw)\n\u001b[0;32m     37\u001b[0m         img\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39murl\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.jpg\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[39mprint\u001b[39m(os\u001b[39m.\u001b[39mlistdir(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\KOSEN-KMUTT\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:3008\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3006\u001b[0m \u001b[39mfor\u001b[39;00m message \u001b[39min\u001b[39;00m accept_warnings:\n\u001b[0;32m   3007\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(message)\n\u001b[1;32m-> 3008\u001b[0m \u001b[39mraise\u001b[39;00m UnidentifiedImageError(\n\u001b[0;32m   3009\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcannot identify image file \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (filename \u001b[39mif\u001b[39;00m filename \u001b[39melse\u001b[39;00m fp)\n\u001b[0;32m   3010\u001b[0m )\n",
      "\u001b[1;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x0000024AFF7AB3D0>"
     ]
    }
   ],
   "source": [
    "total_img = 2000\n",
    "\n",
    "print('single-threading')\n",
    "print(\"Starting...\")\n",
    "start = time.time()\n",
    "web = google_webscraping(path=\"pirate_ship_images\",\n",
    "                         query=\"pirate ship\", total_img=total_img)\n",
    "for start_index in range(0, web.total_img, 20):\n",
    "    web.run(start_index)\n",
    "old = time.time()-start\n",
    "print(\"Time taken [s]:\", old)\n",
    "web.close\n",
    "\n",
    "print('multi-threading')\n",
    "print(\"Starting...\")\n",
    "start = time.time()\n",
    "web = google_webscraping(path=\"pirate_ship_images\",\n",
    "                         query=\"pirate ship\", total_img=total_img)\n",
    "web.start()\n",
    "new = time.time()-start\n",
    "print(\"Time taken [s]:\", new)\n",
    "web.close()\n",
    "\n",
    "print(\"Speedup [เท่าตัว]:\", old/new)\n",
    "print(\"Average Time per image old [s]:\", old/total_img)\n",
    "print(\"Average Time per image new [s]:\", new/total_img)\n",
    "print(\"Faster time per image [s]:\", (old-new)/total_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7566c357a2ac7a066fbcdaf983ecb406f1f4f36b82343b2aebd9c737a9a92978"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
